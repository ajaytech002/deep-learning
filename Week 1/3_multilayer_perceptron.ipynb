{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook is copyright &copy; of <a href=\"https://ajaytech.co\">Ajay Tech</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last chapter, we have seen how a single perceptron can solve a linearly separable data classification problem. Let's check out a small problem that is not linearly separable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for XOR gate looks like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## XOR gate data\n",
    "import numpy as np\n",
    "\n",
    "x = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "\n",
    "# numpy has a built in operator called logical_xor that can calculate the XOR operation of two arrays.\n",
    "# This returns a True or False \n",
    "y = np.logical_xor(x[:,0], x[:,1])\n",
    "\n",
    "# convert the returned True/False to integers\n",
    "y = y.astype(int)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this out and see if the data is linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x200f38d87f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.scatter(x[:,0],x[:,1],c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from this visual, you cannot use a single hyperplane (a line in case of 2-d data) to classify the XOR data. So, by definition, a single perceptron cannot solve this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/xor-data-not-linearly-separable.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, a combination of perceptrons can solve this. Let's code this in Python. This time, let's write this as a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TwoLayerPerceptron :\n",
    "    \n",
    "    def __init__ (self,input_nodes, hidden_nodes, \n",
    "                  output_nodes, learning_rate = 0.001) :\n",
    "        \n",
    "        # structure of the NN. These variables represent the number of nodes\n",
    "        # in each of the layers of the NN\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        # learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # weights matrix\n",
    "        # ih = input-> hidden & ho = hidden-> output\n",
    "        self.weights_ih = np.random.normal(loc = 0.0, \n",
    "                                           scale = 2,\n",
    "                                           size = (self.input_nodes, self.hidden_nodes))\n",
    "        self.weights_ho = np.random.normal(loc = 0.0, \n",
    "                                           scale = 2,\n",
    "                                           size = (self.hidden_nodes, self.output_nodes))        \n",
    "    def forward_prop(self, record) :\n",
    "        #input->hidden layer\n",
    "        weighted_sum  = np.dot(record,self.weights_ih)\n",
    "        # apply activation function in the hidden layer\n",
    "        hidden_output = weighted_sum >=0  # this returns a True or False\n",
    "        self.hidden_output = hidden_output.astype(int)\n",
    "        \n",
    "        #hidden->output layer\n",
    "        weighted_sum  = np.dot(hidden_output,self.weights_ho)\n",
    "        #apply activation function in the output layer\n",
    "        output_output = weighted_sum >=0  \n",
    "        output_output = output_output.astype(int)\n",
    "        self.y_hat = output_output\n",
    "    \n",
    "    def backward_prop(self, record, index) :\n",
    "        \n",
    "        # we start off with the error in the output layer(y - y_hat)\n",
    "        output_error = y[index] - self.y_hat\n",
    "        \n",
    "        # update the weights between the hidden layer and output layer\n",
    "        self.weights_ho += self.learning_rate * np.dot( np.transpose(self.hidden_output), output_error) \n",
    "        \n",
    "        # error in the hidden layer\n",
    "        hidden_error = np.dot(self.weights_ho.T, output_error)\n",
    "        \n",
    "        # update the weights between the input layer and hidden layer\n",
    "        self.weights_ih += self.learning_rate * np.dot(np.transpose(record), hidden_error)\n",
    "        pass\n",
    "\n",
    "        # this is where training happens\n",
    "    def fit(self,X,y,epoch = 1000) :\n",
    "        for i in range(epoch) :\n",
    "            # run forward and back prop for each row of data\n",
    "            for index, record in enumerate(X):\n",
    "                self.forward_prop(record)\n",
    "                self.backward_prop(record, index)\n",
    "        pass\n",
    "    \n",
    "    # given the input data, predict the output by doing forward_prop        \n",
    "    def predict(X):\n",
    "        \n",
    "        y = [] # to hold the predicted output        \n",
    "        for record in X :\n",
    "            y.append(self.forward_prop(record))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,) and (1,) not aligned: 2 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0a75431b99b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTwoLayerPerceptron\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-15-9cc4453d0fa6>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, epoch)\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-9cc4453d0fa6>\u001b[0m in \u001b[0;36mbackward_prop\u001b[1;34m(self, record, index)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# update the weights between the hidden layer and output layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights_ho\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# error in the hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (2,) and (1,) not aligned: 2 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "nn = TwoLayerPerceptron(2,2,1)\n",
    "nn.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wih = np.random.normal(loc = 0.0, scale = 2,size = (3, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.25132759, -1.69621472],\n",
       "       [ 1.73657056, -1.92146063],\n",
       "       [ 2.07086691, -0.79877659]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7320508075688772"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pow(3, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  #### $w_{1 new} = w_{1 old} + \\alpha \\times x_1 \\times (y - \\hat y) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2]\n",
    "\n",
    "b = np.array(a,ndmin=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
