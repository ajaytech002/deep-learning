{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook is copyright &copy; of <a href=\"https://ajaytech.co\">Ajay Tech</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- iris Classification using Tensorflow & Keras\n",
    "  - Step 1 - What kind of Neural Network are we building ?\n",
    "  - Step 2 - How are the neurons connected ?\n",
    "  - Step 3 - Hidden Layers\n",
    "  - Step 4 - Output Layer\n",
    "  - Step 5 - Compile the model\n",
    "  - Step 6 - Fit the model with training data\n",
    "  - Step 7 - Predict data\n",
    "  - Step 8 - Evaluate Model\n",
    "  - Step 9 - Optimize Model \n",
    "- MNIST handwritten digits classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iris Classification using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many times, the MNIST database (a database of handwritten digits) is typically the _Hello World_ application when introducing Neural Networks for the first time. However, we are going to make it even simpler by taking the iris dataset and create a keras based tensorflow Neural Network to classify species. Please refer to <a href=\"https://ajaytech.co/python-classification/#iris-dataset\"> Iris Data </a> to understand more about the dataset we will be working on. You can also refer to <a href=\"https://ajaytech.co/python-classification\"> Classification in Python </a> to understand more about a non-neural network based classification approach to classifying the species in the iris dataset. \n",
    "\n",
    "Once you understand how to solve the iris classification problem in Neural Networks, we will move to image recognition. As you will see, structurally there is not a lot of difference in the way we build the neural net for both of these problems.\n",
    "\n",
    "**This is just a \"Hello World\" tutorial. It is not intended to teach you the internals of Neural Networks**. With that background, we are now ready to say _hello_ to Neural Networks using Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from   tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow is probably the most popular, open source library from Google that is used to implement Deep Learning. You can build neural networks of any complexity using Tensorflow. However, building a neural net from scratch typically involves defining\n",
    "\n",
    "- Layers\n",
    "- Linking the layers\n",
    "- loss function\n",
    "- weight adjustments etc\n",
    "\n",
    "Defining these manually is very time consuming and daunting for newbies. What is needed is an abstract layer above Tensorflow, that makes building neural nets much quicker and easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is the answer. Keras is a high level Python based API that can be used to build neural nets by leveraging Tensorflow. By the way, Tensorflow is not the only deep learning package out there. Here is a quick visual that shows you where Keras and Tensorflow stand in the hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/keras-vs-lower-level-dl-libraries.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n",
      "[0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# load iris dataset\n",
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# preview the iris data\n",
    "print ( iris.data[0:5,:]  ) # data\n",
    "print ( iris.target[0:5]  ) # target species\n",
    "\n",
    "# train/test split @ 20% test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data , iris.target, test_size=0.2)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following 8 lines of code is all you need to solve the problem. Quickly execute it to see the output for yourself. However, there is quite a lot of explantion to be done here. Let's take it step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "120/120 [==============================] - 0s 521us/sample - loss: 1.6050 - acc: 0.2917\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.5040 - acc: 0.2917\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.4096 - acc: 0.2917\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 1.3262 - acc: 0.4333\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 1.2607 - acc: 0.5667\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 1.2080 - acc: 0.4667\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.1707 - acc: 0.4917\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 1.1451 - acc: 0.5000\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.1258 - acc: 0.5167\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s 62us/sample - loss: 1.1068 - acc: 0.5417\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 0s 54us/sample - loss: 1.0904 - acc: 0.5833\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.0770 - acc: 0.5833\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.0674 - acc: 0.5250\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.0610 - acc: 0.3917\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.0548 - acc: 0.3583\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.0498 - acc: 0.3417\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.0453 - acc: 0.3417\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s 54us/sample - loss: 1.0397 - acc: 0.3417\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.0344 - acc: 0.3417\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 1.0294 - acc: 0.3417\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 1.0241 - acc: 0.3417\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 1.0185 - acc: 0.3417\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.0128 - acc: 0.3417\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.0065 - acc: 0.3417\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.9998 - acc: 0.3417\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s 54us/sample - loss: 0.9933 - acc: 0.3417\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9869 - acc: 0.3583\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.9786 - acc: 0.3667\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9715 - acc: 0.4250\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9627 - acc: 0.5500\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9554 - acc: 0.6083\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.9456 - acc: 0.6250\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s 42us/sample - loss: 0.9367 - acc: 0.6250\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9272 - acc: 0.6333\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9173 - acc: 0.6333\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9076 - acc: 0.6500\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.8972 - acc: 0.6500\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.8869 - acc: 0.6500\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.8768 - acc: 0.6583\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.8661 - acc: 0.6583\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.8548 - acc: 0.6583\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.8442 - acc: 0.6583\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.8339 - acc: 0.6667\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.8220 - acc: 0.6667\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.8111 - acc: 0.6667\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s 58us/sample - loss: 0.7997 - acc: 0.6750\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.7883 - acc: 0.6833\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.7770 - acc: 0.6833\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.7658 - acc: 0.6750\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.7541 - acc: 0.6750\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.7431 - acc: 0.6917\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.7317 - acc: 0.6917\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.7211 - acc: 0.7167\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.7099 - acc: 0.7250\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.6991 - acc: 0.7167\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.6885 - acc: 0.7167\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 0.6782 - acc: 0.7083\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.6684 - acc: 0.7083\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.6599 - acc: 0.7167\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.6481 - acc: 0.7667\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.6382 - acc: 0.7583\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.6286 - acc: 0.7750\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.6196 - acc: 0.7667\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s 58us/sample - loss: 0.6111 - acc: 0.7667\n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.6018 - acc: 0.7833\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.5936 - acc: 0.7917\n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.5860 - acc: 0.8000\n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s 25us/sample - loss: 0.5769 - acc: 0.8250\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.5688 - acc: 0.8167\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.5610 - acc: 0.8250\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.5537 - acc: 0.8417\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s 54us/sample - loss: 0.5461 - acc: 0.8500\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.5397 - acc: 0.8417\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.5323 - acc: 0.8417\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.5266 - acc: 0.8500\n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.5184 - acc: 0.8583\n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.5126 - acc: 0.8583\n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.5079 - acc: 0.8500\n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.5031 - acc: 0.8583\n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.4950 - acc: 0.8500\n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.4883 - acc: 0.8500\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4828 - acc: 0.8583\n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4782 - acc: 0.8583\n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4727 - acc: 0.8583\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4672 - acc: 0.8750\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4621 - acc: 0.8750\n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 0.4567 - acc: 0.8750\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 0.4513 - acc: 0.8750\n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4470 - acc: 0.8833\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4426 - acc: 0.8917\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4374 - acc: 0.8833\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4326 - acc: 0.8750\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.4277 - acc: 0.8833\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4234 - acc: 0.8833\n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4188 - acc: 0.8917\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4139 - acc: 0.8917\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.4095 - acc: 0.8917\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 0.4054 - acc: 0.8917\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4006 - acc: 0.8917\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3962 - acc: 0.8917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.20811760e-02, 3.91645938e-01, 5.96272826e-01],\n",
       "       [1.69933531e-02, 3.97484392e-01, 5.85522234e-01],\n",
       "       [9.59960818e-01, 3.92647162e-02, 7.74465443e-04],\n",
       "       [1.90705255e-01, 6.64598465e-01, 1.44696265e-01],\n",
       "       [3.11258342e-03, 2.26298794e-01, 7.70588636e-01],\n",
       "       [8.99805903e-01, 9.60557535e-02, 4.13831137e-03],\n",
       "       [6.15397003e-03, 2.78556108e-01, 7.15289891e-01],\n",
       "       [9.58240926e-01, 4.09574024e-02, 8.01660179e-04],\n",
       "       [8.35558847e-02, 6.37535155e-01, 2.78908908e-01],\n",
       "       [9.11576152e-01, 8.53549615e-02, 3.06887995e-03],\n",
       "       [2.80173123e-02, 5.20794570e-01, 4.51188117e-01],\n",
       "       [9.81949151e-01, 1.78453047e-02, 2.05568867e-04],\n",
       "       [9.13475394e-01, 8.33630040e-02, 3.16164969e-03],\n",
       "       [4.98204343e-02, 5.69957256e-01, 3.80222321e-01],\n",
       "       [2.83193532e-02, 5.36988616e-01, 4.34692025e-01],\n",
       "       [6.19469536e-03, 2.78104872e-01, 7.15700507e-01],\n",
       "       [5.04648834e-02, 5.63345432e-01, 3.86189699e-01],\n",
       "       [9.01798606e-01, 9.46312845e-02, 3.57011799e-03],\n",
       "       [3.41202389e-03, 2.44403824e-01, 7.52184212e-01],\n",
       "       [9.06935573e-01, 9.03311223e-02, 2.73334724e-03],\n",
       "       [9.46662784e-01, 5.19549623e-02, 1.38220214e-03],\n",
       "       [9.40084696e-01, 5.81936389e-02, 1.72167330e-03],\n",
       "       [9.40235198e-01, 5.79454526e-02, 1.81935111e-03],\n",
       "       [9.38879550e-01, 5.96898273e-02, 1.43059052e-03],\n",
       "       [9.09764946e-01, 8.71445313e-02, 3.09041399e-03],\n",
       "       [9.40479219e-01, 5.79398200e-02, 1.58103404e-03],\n",
       "       [2.62060165e-01, 6.20771348e-01, 1.17168434e-01],\n",
       "       [4.93753655e-03, 2.82564163e-01, 7.12498307e-01],\n",
       "       [9.47779417e-01, 5.09882867e-02, 1.23227667e-03],\n",
       "       [1.00235706e-02, 3.16601396e-01, 6.73375070e-01]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(4,input_shape=(4,)))\n",
    "model.add(keras.layers.Dense(8,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(8,activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(3,activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 - What type of neural network are we building ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two types of Neural networks that can be build in Keras\n",
    "\n",
    "- Sequential\n",
    "- Functional\n",
    "\n",
    "This classification is related to the structure of the Neural Network. However, most of the time you will be using Sequential model. It can solve most of the problems. In a sequential neural net, neurons are arranged in layers and in _sequence_ . The firing and wiring happen in _sequence_, hence the name. Later in the course when we see an example of a functional neural net, the difference will be clear. Here is a quick visual of what we are building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/neural-network-layers-sequential.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, when the network is trained, the outputs corresponding to the species (for the corresponding data points) will light-up. When we look at the last step, we will understand what I meant by _light-up_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 - How are the neurons connected ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are building a **Dense** neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(4,input_shape=(4,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A Dense neural network is one in which each neuron is connected to all other neurons in the previous and next layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the visual below that the arrows coming in to each neuron are connected to all the neurons in the previous layer. This is the most typical type of neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/keras-input-layer.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, with this statement, we are just building the input layer. An input layer with 4 nodes, one node for each of the inputs. Naturally, the assumption at this point would be that there would be as many nodes in the input layer as the number of inputs. So, why specify the **input_shape** parameter again ? In later examples we will see that the input data shape need not always match with the input nodes. We specify the input_shape parameter as a tuple. In this case the input is a 1-d vector. Once again, later in the course we will see examples of 2-d data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter **input_shape** is only used when creating the first layer. The next set of steps (hidden layer and output layer) do not need this parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 - Hidden Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the magic happens. Let's try it with just one hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(8,activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/keras-hidden-layer.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Irrespective of the layer (input, hidden or output), the way to add layers is using the **add** function. That should make things easy for us. The new parameter that you see in the hidden layer is the **activation** parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/keras-activation-function.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are using a **relu** activation function. **ReLU** stands for _Rectified Linear Unit_. The mathematical definition of _relu_ is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $y = max(0,x) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the activation function looks like this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/relu-activation-function.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 - Output Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the hidden layer is added, we add the output layer. Since we are doing a multi-class classification, the preferred activation function is called as a softmax - more on this later. A softmax activation function gives out multiple probability values and the one with the highest probability is the predicted output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(3,activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/output-layer.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5 - Compile the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have created the structure of the neural net - layer by layer. At each step, we have defined the number of nodes and the activation function to be used. Once we have completed it, we now have to compile the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have just defined how the neural net should look like. With the **compile ( )** method, Keras translates the parameters you have specified into an optimized series of steps that can then be executed on the computer. Without the _compile_ step, you cannot fit (train) the model. We will see how we can use metrics in a bit, but optimizer and loss parameters requrie quite a bit of explanation. \n",
    "\n",
    "Typically, Machine Learning algorithm requires some kind of a loss function to be minimized. Gradient Descent is a commonly used loss function. For classification problems, a common loss function is **Cross Entropy**. _Cross Entropy_ is also called as **Log Loss**. Mathematically, this is a how a cross entropy function can be defined for 2 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L_{Cross Entropy} = -(y \\log{p} + (1-y)\\log{(1-p)})$\n",
    "\n",
    "where\n",
    "- $y$ = actual class\n",
    "- $p$ = predicted class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example below. Say, we are just looking at 2 species of iris flowers. \n",
    "\n",
    "0 - setosa\n",
    "1 - virginica\n",
    "\n",
    "If the model has predicted the species to be setosa with a probability of 0.2, the loss function can be calculated as follows. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $L_{Cross Entropy} = -(0 \\log{0.2} + (1-0)\\log{(1-0.2)}) = -(\\log{0.8}) = - (-0.096) = 0.096$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss function')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "p = np.array([0.0001,0.001,0.01,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,0.99])\n",
    "\n",
    "# when y = 1, what is the loss function ?\n",
    "y = 1\n",
    "l = - (y * np.log10(p) + (1-y) * np.log10(1-p)  )\n",
    "\n",
    "# now plot it to see how the loss function decreases as the predicted value approaches the actual value (of y = 1)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.scatter(p,l)\n",
    "plt.xlabel(\"Different predicted values when the actual value is 1\")\n",
    "plt.ylabel(\"Loss function\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this plot means is that the more the predicted value deviates from the actual value, the more the loss function is. For example, when the predicted value reaches close to the actual value (of 1 in this case), the loss function gets closer and closer to 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you can see a quick summary of the model you have created so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 27        \n",
      "=================================================================\n",
      "Total params: 87\n",
      "Trainable params: 87\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6 - Fit the model with training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we train the dataset. The word **epoch** represents one complete iteration over the training dataset. With each epoc (one pass over the entire dataset) the weights are adjusted and the accuracy slowly increases. Since you have _accuracy_ as a metric in step 5, it is shown at each of the training epoch. That way you see how the accuracy increases with each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "120/120 [==============================] - 0s 446us/sample - loss: 1.6451 - acc: 0.2167\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.5703 - acc: 0.2750\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 1.5049 - acc: 0.3083\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 1.4404 - acc: 0.3083\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 1.3812 - acc: 0.3167\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 1.3288 - acc: 0.3333\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.2814 - acc: 0.3333\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.2384 - acc: 0.3417\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.2007 - acc: 0.3417\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 1.1680 - acc: 0.3500\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.1370 - acc: 0.3917\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.1077 - acc: 0.4583\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 1.0831 - acc: 0.5167\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 1.0567 - acc: 0.5417\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.0347 - acc: 0.5833\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.0128 - acc: 0.6000\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.9897 - acc: 0.6000\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9706 - acc: 0.6167\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s 54us/sample - loss: 0.9528 - acc: 0.6167\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9355 - acc: 0.6167\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.9172 - acc: 0.6083\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9005 - acc: 0.5917\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s 25us/sample - loss: 0.8838 - acc: 0.5833\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.8681 - acc: 0.5917\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.8518 - acc: 0.6417\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.8370 - acc: 0.8000\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.8212 - acc: 0.8500\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.8063 - acc: 0.8750\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.7920 - acc: 0.9000\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.7775 - acc: 0.9000\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.7639 - acc: 0.9083\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.7499 - acc: 0.9000\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.7373 - acc: 0.9000\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.7235 - acc: 0.9000\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.7112 - acc: 0.9000\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.6985 - acc: 0.9000\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.6864 - acc: 0.9083\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.6744 - acc: 0.9250\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.6631 - acc: 0.9333\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.6517 - acc: 0.9333\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s 25us/sample - loss: 0.6411 - acc: 0.9417\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.6303 - acc: 0.9333\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.6197 - acc: 0.9333\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s 46us/sample - loss: 0.6098 - acc: 0.9333\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.6000 - acc: 0.9333\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.5902 - acc: 0.9333\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.5814 - acc: 0.9333\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.5721 - acc: 0.9333\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.5637 - acc: 0.9417\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.5550 - acc: 0.9417\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.5469 - acc: 0.9417\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.5393 - acc: 0.9667\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.5317 - acc: 0.9667\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.5239 - acc: 0.9667\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.5167 - acc: 0.9667\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.5096 - acc: 0.9667\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.5027 - acc: 0.9667\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.4964 - acc: 0.9667\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.4897 - acc: 0.9667\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4837 - acc: 0.9667\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4774 - acc: 0.9667\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4716 - acc: 0.9667\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4660 - acc: 0.9667\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s 54us/sample - loss: 0.4602 - acc: 0.9667\n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4547 - acc: 0.9667\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4495 - acc: 0.9667\n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.4440 - acc: 0.9667\n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4390 - acc: 0.9667\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4339 - acc: 0.9667\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 0.4292 - acc: 0.9667\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.4243 - acc: 0.9667\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.4196 - acc: 0.9667\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4149 - acc: 0.9750\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4105 - acc: 0.9750\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4062 - acc: 0.9750\n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4018 - acc: 0.9750\n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.3976 - acc: 0.9750\n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.3936 - acc: 0.9750\n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3903 - acc: 0.9750\n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3852 - acc: 0.9750\n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3817 - acc: 0.9750\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3781 - acc: 0.9750\n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3740 - acc: 0.9750\n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.3703 - acc: 0.9750\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.3664 - acc: 0.9750\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3630 - acc: 0.9750\n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3591 - acc: 0.9750\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.3557 - acc: 0.9750\n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3523 - acc: 0.9750\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3488 - acc: 0.9750\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3454 - acc: 0.9750\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 0.3422 - acc: 0.9750\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3389 - acc: 0.9750\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3357 - acc: 0.9750\n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3329 - acc: 0.9750\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3296 - acc: 0.9750\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 0s 25us/sample - loss: 0.3267 - acc: 0.9750\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3232 - acc: 0.9750\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3203 - acc: 0.9750\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s 58us/sample - loss: 0.3171 - acc: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20f74983b38>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing 100 epochs, the accuracy is around 69% - not bad for our first attempt. We will enhance it shortly to 90% by just adding one more hidden layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 7 - Predict data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you can start predicting your test data. This step is pretty straight forward if you have already used sklearn to predict test data based on any machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.80523752e-03, 1.61355630e-01, 8.33839059e-01],\n",
       "       [5.65165561e-03, 1.38419360e-01, 8.55928957e-01],\n",
       "       [9.37631130e-01, 5.78489937e-02, 4.51981043e-03],\n",
       "       [9.99230742e-02, 7.17553735e-01, 1.82523206e-01],\n",
       "       [9.50973015e-04, 1.86273277e-01, 8.12775731e-01],\n",
       "       [8.69464993e-01, 1.15551665e-01, 1.49833458e-02],\n",
       "       [6.78034406e-03, 2.44014740e-01, 7.49204934e-01],\n",
       "       [9.12608325e-01, 8.10388103e-02, 6.35283068e-03],\n",
       "       [6.19196370e-02, 7.35043049e-01, 2.03037351e-01],\n",
       "       [8.91312957e-01, 9.75011438e-02, 1.11858230e-02],\n",
       "       [1.72961298e-02, 3.84219527e-01, 5.98484337e-01],\n",
       "       [9.59164917e-01, 3.88094820e-02, 2.02561123e-03],\n",
       "       [9.01787043e-01, 8.78127143e-02, 1.04002040e-02],\n",
       "       [4.25860547e-02, 4.14889425e-01, 5.42524457e-01],\n",
       "       [2.64684074e-02, 6.92561448e-01, 2.80970186e-01],\n",
       "       [3.44479713e-03, 2.71285832e-01, 7.25269318e-01],\n",
       "       [4.74239029e-02, 6.01970136e-01, 3.50605994e-01],\n",
       "       [8.60095024e-01, 1.25155374e-01, 1.47496713e-02],\n",
       "       [1.49409531e-03, 2.42252737e-01, 7.56253123e-01],\n",
       "       [8.37987006e-01, 1.46805704e-01, 1.52072664e-02],\n",
       "       [8.97249877e-01, 9.39026028e-02, 8.84753559e-03],\n",
       "       [9.13183808e-01, 7.87739381e-02, 8.04229267e-03],\n",
       "       [8.98115098e-01, 9.22952741e-02, 9.58956406e-03],\n",
       "       [8.81850123e-01, 1.08397752e-01, 9.75214690e-03],\n",
       "       [8.75269353e-01, 1.11916631e-01, 1.28139891e-02],\n",
       "       [9.16219890e-01, 7.63572678e-02, 7.42286025e-03],\n",
       "       [1.50582254e-01, 6.88981295e-01, 1.60436377e-01],\n",
       "       [8.01152084e-03, 4.72976834e-01, 5.19011676e-01],\n",
       "       [9.26478028e-01, 6.74546957e-02, 6.06729649e-03],\n",
       "       [8.82636756e-03, 2.28457645e-01, 7.62715995e-01]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a multi-class output, what the neural net outputs are probabilities. The highest probability among the three elements is the predicted value. However, we need to convert these probabilities back to indices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/np-argmax.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 1, 2, 0, 2, 0, 1, 0, 2, 0, 0, 2, 1, 2, 1, 0, 2, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 2, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "y_pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 8 - Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the output is categorical data, a quick confusion matrix will show use how far we are from the model. Use scikit learn's confusion matrix should do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  0  0]\n",
      " [ 0  5  1]\n",
      " [ 0  0  9]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "cm = confusion_matrix(y_test, y_pred_class)\n",
    "print ( cm )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course the final number - accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    " \n",
    "accuracy_score(y_test,y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is pretty low by any Machine Learning standards for this dataset. Let's optimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 9 - Optimize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of ways to optimize for higher accuracy. One way is to increase the nodes in the hidden layer. Let's try to increase the number of nodes from 8 to 20 and see how the network performs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "120/120 [==============================] - 0s 484us/sample - loss: 1.9111 - acc: 0.2917\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 1.7657 - acc: 0.2917\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.6348 - acc: 0.2917\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.5117 - acc: 0.2917\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.4100 - acc: 0.2917\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.3202 - acc: 0.2917\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 1.2427 - acc: 0.2917\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.1696 - acc: 0.2917\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 1.1183 - acc: 0.2917\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.0729 - acc: 0.4083\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 1.0347 - acc: 0.4667\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 1.0049 - acc: 0.5000\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.9793 - acc: 0.5500\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.9566 - acc: 0.5917\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.9369 - acc: 0.5833\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9184 - acc: 0.6000\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9012 - acc: 0.6000\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.8860 - acc: 0.6083\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.8717 - acc: 0.6083\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.8571 - acc: 0.6083\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.8440 - acc: 0.6083\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.8301 - acc: 0.6250\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.8142 - acc: 0.6333\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.7965 - acc: 0.6417\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.7782 - acc: 0.6417\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.7605 - acc: 0.6417\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.7451 - acc: 0.6583\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 0.7296 - acc: 0.6667\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.7159 - acc: 0.6917\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.7041 - acc: 0.7417\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.6930 - acc: 0.7833\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.6817 - acc: 0.7833\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.6706 - acc: 0.7750\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.6605 - acc: 0.7833\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.6506 - acc: 0.8250\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.6410 - acc: 0.8083\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.6306 - acc: 0.8000\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.6215 - acc: 0.8500\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.6125 - acc: 0.8583\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.6036 - acc: 0.8833\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.5949 - acc: 0.8833\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s 46us/sample - loss: 0.5869 - acc: 0.8750\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.5784 - acc: 0.8833\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.5705 - acc: 0.8917\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.5629 - acc: 0.8917\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.5553 - acc: 0.8917\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.5480 - acc: 0.8917\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.5409 - acc: 0.8833\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.5341 - acc: 0.8917\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.5273 - acc: 0.8833\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.5213 - acc: 0.9167\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.5150 - acc: 0.8917\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.5080 - acc: 0.8917\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.5011 - acc: 0.9167\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4956 - acc: 0.9250\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.4905 - acc: 0.9250\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.4836 - acc: 0.9250\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4779 - acc: 0.9250\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.4727 - acc: 0.9250\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4669 - acc: 0.9250\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s 25us/sample - loss: 0.4612 - acc: 0.9250\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.4557 - acc: 0.9250\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4508 - acc: 0.9250\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4456 - acc: 0.9250\n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 0s 46us/sample - loss: 0.4406 - acc: 0.9250\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4357 - acc: 0.9250\n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4306 - acc: 0.9333\n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s 46us/sample - loss: 0.4253 - acc: 0.9417\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4216 - acc: 0.9333\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4159 - acc: 0.9250\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.4108 - acc: 0.9417\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.4066 - acc: 0.9417\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4021 - acc: 0.9417\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3976 - acc: 0.9417\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.3932 - acc: 0.9333\n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3888 - acc: 0.9417\n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3840 - acc: 0.9500\n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s 49us/sample - loss: 0.3796 - acc: 0.9417\n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.3755 - acc: 0.9417\n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3714 - acc: 0.9417\n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3671 - acc: 0.9417\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3628 - acc: 0.9500\n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3595 - acc: 0.9417\n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3549 - acc: 0.9500\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.3507 - acc: 0.9417\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3470 - acc: 0.9417\n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3444 - acc: 0.9500\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3406 - acc: 0.9417\n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3351 - acc: 0.9583\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3318 - acc: 0.9583\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3281 - acc: 0.9583\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3252 - acc: 0.9583\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.3211 - acc: 0.9417\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 0.3174 - acc: 0.9500\n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3142 - acc: 0.9500\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3102 - acc: 0.9583\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3081 - acc: 0.9500\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3040 - acc: 0.9583\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.2998 - acc: 0.9667\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.2974 - acc: 0.9500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(4,input_shape=(4,)))\n",
    "# BEGIN change - increase the number of nodes from 8 to 20\n",
    "model.add(keras.layers.Dense(20,activation=\"relu\"))\n",
    "# END change\n",
    "model.add(keras.layers.Dense(3,activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    " \n",
    "accuracy_score(y_test,y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now hit 90% accuracy. That's pretty much close to what most ML models would achieve. Let's try to keep the number of nodes the same, but add one more hidden layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "120/120 [==============================] - 0s 574us/sample - loss: 1.9741 - acc: 0.2333\n",
      "Epoch 2/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.8711 - acc: 0.3417\n",
      "Epoch 3/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.7824 - acc: 0.3417\n",
      "Epoch 4/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.7114 - acc: 0.3417\n",
      "Epoch 5/100\n",
      "120/120 [==============================] - 0s 46us/sample - loss: 1.6428 - acc: 0.3417\n",
      "Epoch 6/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.5780 - acc: 0.3417\n",
      "Epoch 7/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.5234 - acc: 0.3417\n",
      "Epoch 8/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.4688 - acc: 0.3417\n",
      "Epoch 9/100\n",
      "120/120 [==============================] - 0s 54us/sample - loss: 1.4162 - acc: 0.3417\n",
      "Epoch 10/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 1.3703 - acc: 0.3417\n",
      "Epoch 11/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.3316 - acc: 0.3417\n",
      "Epoch 12/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 1.2942 - acc: 0.3417\n",
      "Epoch 13/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.2585 - acc: 0.3417\n",
      "Epoch 14/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.2258 - acc: 0.3417\n",
      "Epoch 15/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.2001 - acc: 0.3417\n",
      "Epoch 16/100\n",
      "120/120 [==============================] - 0s 54us/sample - loss: 1.1726 - acc: 0.3417\n",
      "Epoch 17/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.1488 - acc: 0.3417\n",
      "Epoch 18/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.1179 - acc: 0.3583\n",
      "Epoch 19/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.0965 - acc: 0.3833\n",
      "Epoch 20/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 1.0837 - acc: 0.4167\n",
      "Epoch 21/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.0740 - acc: 0.4583\n",
      "Epoch 22/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.0643 - acc: 0.4583\n",
      "Epoch 23/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.0557 - acc: 0.4667\n",
      "Epoch 24/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 1.0479 - acc: 0.4583\n",
      "Epoch 25/100\n",
      "120/120 [==============================] - 0s 38us/sample - loss: 1.0408 - acc: 0.4500\n",
      "Epoch 26/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.0339 - acc: 0.4333\n",
      "Epoch 27/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 1.0268 - acc: 0.4583\n",
      "Epoch 28/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.0209 - acc: 0.4417\n",
      "Epoch 29/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 1.0147 - acc: 0.4583\n",
      "Epoch 30/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 1.0085 - acc: 0.4667\n",
      "Epoch 31/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 1.0032 - acc: 0.4917\n",
      "Epoch 32/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 0.9976 - acc: 0.5167\n",
      "Epoch 33/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.9920 - acc: 0.5250\n",
      "Epoch 34/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.9871 - acc: 0.5167\n",
      "Epoch 35/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.9814 - acc: 0.5333\n",
      "Epoch 36/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 0.9770 - acc: 0.5250\n",
      "Epoch 37/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.9714 - acc: 0.5333\n",
      "Epoch 38/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.9655 - acc: 0.5333\n",
      "Epoch 39/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9607 - acc: 0.5250\n",
      "Epoch 40/100\n",
      "120/120 [==============================] - 0s 46us/sample - loss: 0.9554 - acc: 0.5333\n",
      "Epoch 41/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.9513 - acc: 0.5333\n",
      "Epoch 42/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9445 - acc: 0.5417\n",
      "Epoch 43/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9397 - acc: 0.5333\n",
      "Epoch 44/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9336 - acc: 0.5417\n",
      "Epoch 45/100\n",
      "120/120 [==============================] - 0s 54us/sample - loss: 0.9280 - acc: 0.5500\n",
      "Epoch 46/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.9226 - acc: 0.5417\n",
      "Epoch 47/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.9163 - acc: 0.5500\n",
      "Epoch 48/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 0.9104 - acc: 0.5500\n",
      "Epoch 49/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.9049 - acc: 0.5500\n",
      "Epoch 50/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.8987 - acc: 0.5500\n",
      "Epoch 51/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.8940 - acc: 0.5500\n",
      "Epoch 52/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.8874 - acc: 0.5500\n",
      "Epoch 53/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.8819 - acc: 0.5500\n",
      "Epoch 54/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.8735 - acc: 0.5500\n",
      "Epoch 55/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.8543 - acc: 0.5500\n",
      "Epoch 56/100\n",
      "120/120 [==============================] - 0s 54us/sample - loss: 0.8298 - acc: 0.6083\n",
      "Epoch 57/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.8125 - acc: 0.8333\n",
      "Epoch 58/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.7931 - acc: 0.8667\n",
      "Epoch 59/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.7758 - acc: 0.9000\n",
      "Epoch 60/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.7576 - acc: 0.8917\n",
      "Epoch 61/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.7374 - acc: 0.9083\n",
      "Epoch 62/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.7191 - acc: 0.9000\n",
      "Epoch 63/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.7007 - acc: 0.9000\n",
      "Epoch 64/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 0.6813 - acc: 0.9000\n",
      "Epoch 65/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.6627 - acc: 0.9167\n",
      "Epoch 66/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.6429 - acc: 0.9250\n",
      "Epoch 67/100\n",
      "120/120 [==============================] - 0s 45us/sample - loss: 0.6240 - acc: 0.9250\n",
      "Epoch 68/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.6069 - acc: 0.9333\n",
      "Epoch 69/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.5888 - acc: 0.9333\n",
      "Epoch 70/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.5711 - acc: 0.9333\n",
      "Epoch 71/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.5546 - acc: 0.9333\n",
      "Epoch 72/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.5388 - acc: 0.9333\n",
      "Epoch 73/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.5228 - acc: 0.9417\n",
      "Epoch 74/100\n",
      "120/120 [==============================] - 0s 50us/sample - loss: 0.5071 - acc: 0.9417\n",
      "Epoch 75/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.4926 - acc: 0.9417\n",
      "Epoch 76/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4793 - acc: 0.9333\n",
      "Epoch 77/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4666 - acc: 0.9417\n",
      "Epoch 78/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.4537 - acc: 0.9417\n",
      "Epoch 79/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4419 - acc: 0.9417\n",
      "Epoch 80/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.4310 - acc: 0.9417\n",
      "Epoch 81/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.4205 - acc: 0.9417\n",
      "Epoch 82/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.4103 - acc: 0.9500\n",
      "Epoch 83/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.4006 - acc: 0.9500\n",
      "Epoch 84/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.3916 - acc: 0.9500\n",
      "Epoch 85/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3833 - acc: 0.9500\n",
      "Epoch 86/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3753 - acc: 0.9500\n",
      "Epoch 87/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3673 - acc: 0.9500\n",
      "Epoch 88/100\n",
      "120/120 [==============================] - 0s 54us/sample - loss: 0.3601 - acc: 0.9667\n",
      "Epoch 89/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3529 - acc: 0.9667\n",
      "Epoch 90/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3459 - acc: 0.9500\n",
      "Epoch 91/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.3391 - acc: 0.9500\n",
      "Epoch 92/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3339 - acc: 0.9583\n",
      "Epoch 93/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3268 - acc: 0.9583\n",
      "Epoch 94/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3209 - acc: 0.9500\n",
      "Epoch 95/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.3152 - acc: 0.9500\n",
      "Epoch 96/100\n",
      "120/120 [==============================] - 0s 41us/sample - loss: 0.3095 - acc: 0.9500\n",
      "Epoch 97/100\n",
      "120/120 [==============================] - 0s 33us/sample - loss: 0.3042 - acc: 0.9583\n",
      "Epoch 98/100\n",
      "120/120 [==============================] - 0s 37us/sample - loss: 0.2990 - acc: 0.9583\n",
      "Epoch 99/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.2942 - acc: 0.9667\n",
      "Epoch 100/100\n",
      "120/120 [==============================] - 0s 29us/sample - loss: 0.2887 - acc: 0.9583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Dense(4,input_shape=(4,)))\n",
    "model.add(keras.layers.Dense(8,activation=\"relu\"))\n",
    "# BEGIN Change - add one more hidden layer\n",
    "model.add(keras.layers.Dense(8,activation=\"relu\"))\n",
    "# END Change\n",
    "\n",
    "model.add(keras.layers.Dense(3,activation=\"softmax\"))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_class = np.argmax(y_pred,axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    " \n",
    "accuracy_score(y_test,y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's 80% accuracy. The immediate question you might have is - How should you choose the number of nodes or the number of hidden layers ? Unfortunately, the meaning of weights and outputs is essentially a blackbox to humans. Meaning, we cannot make any sense out of it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Choosing the size and complexity of a neural network (like the numbner of nodes and the number of hidden layers) is more art than science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST handwritten digits classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all we had to do in Neural Networks was classify iris data, we wouldn't be needing Neural networks to start with. We need a more involved dataset to quality as a \"Hello World\" program in Neural Networks. Welcome the MNIST digits dataset. It is a dataset of handwritten images that are scanned, standardized and optimized for machine learning. Tensorflow comes built-in with this dataset. Let's quickly load it to see how these images look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "mnist = keras.datasets.mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20f763cbcf8>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show the first picture in the training dataset.\n",
    "plt.figure()\n",
    "plt.imshow(train_images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a handwritten digit - 5. Although the image looks in color, these are actually gray-scale images. All of the data is standardized into 28x28 pixels. And each pixel has an intensity value between 0 and 255 ( 0 - 2<sup>8</sup> ). Since this is a small image (just 28x28 pixels), we can write it to an excel file and see the numbers visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'image_1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-68-3e86563d8a79>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# write the first image to a csv file.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"image_1.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msavetxt\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36msavetxt\u001b[1;34m(fname, X, fmt, delimiter, newline, header, footer, comments, encoding)\u001b[0m\n\u001b[0;32m   1362\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m         \u001b[1;31m# datasource doesn't support creating a new file ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1364\u001b[1;33m         \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1365\u001b[0m         \u001b[0mfh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m         \u001b[0mown_fh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'image_1.csv'"
     ]
    }
   ],
   "source": [
    "# write the first image to a csv file.\n",
    "np.savetxt(\"image_1.csv\",train_images[0],delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you open the csv file in excel, and adjust the column size and change the number format to zero decimals, you should see a picture like this. Can you identify the digit 5 in there ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/mnist-image-in-excel.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that the image is in fact a 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first label\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the model. The first layer in this case is slightly different from the first layer in the iris example above. As you can see from the input image data, it is a 28x28 dimension numpy array. However, we are going to be working with flat (a flat set of neurons in each layer). So, the first layer will be essentially a 784 (28 x 28 = 784) node layer that will be created automatically by flattening the input array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/keras-flatten.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images, train_labels, epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is a vector of 10 x 1 dimension of probabilities. The value with the highest probability in the output array is the predicted value. For example, let's see what is the first image in the test_image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a 7. Let's see how the probabilities are predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"predictions.csv\",predictions[0:10],delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_final[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the array above that the highest probability value is at location 7. Let's apply the argmax function of numpy to just filter out the values with the highest probability into a new array. For example, the first output array has an **argmax** output of 7 ( **argmax ( )** outputs the index of the highest element in a numpy array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./pics/argmax.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the argmax on the output array to predicted array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_final = np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    " \n",
    "accuracy_score(test_labels,predicted_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a 95% accuracy on the test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great !! Our \"Hello World\" of Neural Networks is complete. The next couple of days we will focus on the moving parts of a neural network and how Gradient Descent is used in Neural networks to optimize the weights. This is how the neural network essentially learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
